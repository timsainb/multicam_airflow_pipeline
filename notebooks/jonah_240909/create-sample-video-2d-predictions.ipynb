{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "33acc1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: dataPy_NWB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15aad04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a96c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter binary location: /home/jop9552/miniconda3/envs/dataPy_NWB2/bin/python\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import median_filter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "print(\"Python interpreter binary location:\", sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87e636",
   "metadata": {},
   "source": [
    "NB: this currently expects one video per session. Making QC vids with multiple one-min vids per session is harder.\n",
    "\n",
    "# TODO\n",
    "* allow user to specify start frame, so that we can look in middle of session as well as beginning\n",
    "* add to kpt pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c1ed2",
   "metadata": {},
   "source": [
    "# Pipeline testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466789b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter binary location: /home/jop9552/miniconda3/envs/dataPy_NWB2/bin/python\n"
     ]
    }
   ],
   "source": [
    "# from multicamera_airflow_pipeline.tim_240731.keypoints.validation_videos import KeypointVideoCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2869048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter binary location: /home/jop9552/miniconda3/envs/dataPy_NWB2/bin/python\n"
     ]
    }
   ],
   "source": [
    "# base_dir = \"/n/groups/datta/kpts_pipeline/tim_240731/results\"\n",
    "# session = \"24-09-29-12-40-04-238868\"\n",
    "# predictions_2d_directory = join(base_dir, \"2D_predictions\", session)\n",
    "# camera_calibration_directory = join(base_dir, \"camera_calibration\", \"24-09-29-13-56-13-243339/jarvis/CalibrationParameters\")\n",
    "# output_directory_keypoint_vids = join(base_dir, \"keypoint_validation_videos\", session)\n",
    "# raw_video_directory = \"/n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240929_J07901_6cam_PBN/24-09-29-12-40-04-238868\"\n",
    "# k = KeypointVideoCreator(\n",
    "#     predictions_2d_directory,\n",
    "#     camera_calibration_directory,\n",
    "#     raw_video_directory,\n",
    "#     output_directory_keypoint_vids,\n",
    "#     max_frames=120,\n",
    "#     recompute_completed=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "626d8cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ae5fa7b7a44da68d5c20df32892a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/kpts_pipeline/tim_240731/results/keypoint_validation_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ec9c4ad29b4870be6d8e32f8d27adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/kpts_pipeline/tim_240731/results/keypoint_validation_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f361a49503064cf1b87a42290c49a4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/kpts_pipeline/tim_240731/results/keypoint_validation_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackRight.0_with_2D_keypoints.mp4\n",
      "Total frames: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc459228b23445a7a723c9edb82c409b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/kpts_pipeline/tim_240731/results/keypoint_validation_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9135e2d232443719a42e64f5b7e0bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/kpts_pipeline/tim_240731/results/keypoint_validation_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e508298f563f420b9a0840760b149d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/kpts_pipeline/tim_240731/results/keypoint_validation_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontRight.0_with_2D_keypoints.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# k.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd0339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5dc2a22",
   "metadata": {},
   "source": [
    "# generate_keypoint_video function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a64d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keypoint_video(\n",
    "    output_directory: Path,\n",
    "    video_path: Path,\n",
    "    keypoint_coords: np.ndarray,\n",
    "    keypoint_conf: np.ndarray,  # New parameter for keypoint confidence\n",
    "    keypoint_info: dict,\n",
    "    skeleton_info: dict,\n",
    "    vid_suffix: str,\n",
    "    detection_coords: np.ndarray=None,\n",
    "    max_frames=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a video with keypoint predictions overlaid on the original video frames.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_directory : Path\n",
    "        Directory where the output video will be saved.\n",
    "\n",
    "    video_path : Path\n",
    "        Path to the input video file.\n",
    "\n",
    "    keypoint_coords : np.ndarray\n",
    "        Array of shape (#frames, #keypoints, 2) containing the coordinates of keypoints for each frame.\n",
    "\n",
    "    keypoint_conf : np.ndarray\n",
    "        Array of shape (#frames, #keypoints) containing the confidence values (0-1) for each keypoint in each frame.\n",
    "\n",
    "    keypoint_info : dict\n",
    "        Dictionary containing information about the keypoints. Each key in the dictionary represents a keypoint ID, and the\n",
    "        value is another dictionary with the following structure:\n",
    "        {\n",
    "            'name': str,       # Keypoint name\n",
    "            'id': int,         # Keypoint ID\n",
    "            'color': list,     # RGB color for the keypoint [R, G, B]\n",
    "            'type': str,       # Keypoint type (e.g., 'upper', 'lower')\n",
    "            'swap': str        # Name of the corresponding left/right keypoint to be swapped (for symmetry)\n",
    "        }\n",
    "\n",
    "    skeleton_info : dict\n",
    "        Dictionary containing information about the skeleton. Each key in the dictionary represents a skeleton link ID, and\n",
    "        the value is another dictionary with the following structure:\n",
    "        {\n",
    "            'link': tuple,     # Tuple containing the names of the two keypoints that form the link\n",
    "            'id': int,         # Link ID\n",
    "            'color': list      # RGB color for the link [R, G, B]\n",
    "        }\n",
    "\n",
    "    vid_suffix : str\n",
    "        Suffix to add to the video file name. Ie \"with_2D_keypoints\" or \"with_3D_keypoints\"\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function saves the output video with keypoints and skeletons overlaid to the specified output directory.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If the input video cannot be opened.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    output_directory = Path('/output/directory')\n",
    "    video_path = Path('/path/to/video.mp4')\n",
    "    keypoint_coords = np.load('keypoint_coords.npy')  # Load your keypoints array\n",
    "    keypoint_conf = np.load('keypoint_conf.npy')  # Load your keypoint confidence array\n",
    "    keypoint_info = {\n",
    "        0: {'name': 'nose_tip', 'id': 0, 'color': [120, 184, 181], 'type': 'upper', 'swap': ''},\n",
    "        # Add other keypoints as needed\n",
    "    }\n",
    "    skeleton_info = {\n",
    "        0: {'link': ('tail_base', 'spine_low'), 'id': 0, 'color': [173, 160, 183]},\n",
    "        # Add other links as needed\n",
    "    }\n",
    "\n",
    "    generate_keypoint_video(output_directory, video_path, keypoint_coords, keypoint_conf, keypoint_info, skeleton_info)\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create the VideoWriter object\n",
    "    output_path = output_directory / (video_path.stem + \"_\" + vid_suffix + \".mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    if total_frames < 0 and max_frames is None:\n",
    "        raise ValueError(\n",
    "            \"Could not determine total number of frames in the video -- please specify max_frames.\"\n",
    "        )\n",
    "    elif total_frames < 0:\n",
    "        total_frames = max_frames\n",
    "    elif max_frames is not None:\n",
    "        total_frames = np.min([max_frames, total_frames])\n",
    "\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Create an overlay for drawing\n",
    "            overlay = frame.copy()\n",
    "\n",
    "            # Draw keypoints\n",
    "            for kp_idx, kp_info in keypoint_info.items():\n",
    "                if (\n",
    "                    frame_idx < len(keypoint_coords)\n",
    "                    and kp_idx < keypoint_coords.shape[1]\n",
    "                ):\n",
    "                    x, y = keypoint_coords[frame_idx, kp_idx]\n",
    "                    if np.isnan(x) or np.isnan(y):\n",
    "                        continue\n",
    "                    conf = keypoint_conf[frame_idx, kp_idx]\n",
    "                    color = tuple(kp_info[\"color\"])\n",
    "                    alpha = conf  # Alpha value is based on the confidence (0-1)\n",
    "                    if conf > 0:  # Only draw if confidence is greater than 0\n",
    "                        overlay = cv2.circle(\n",
    "                            overlay,\n",
    "                            (int(x), int(y)),\n",
    "                            radius=4,\n",
    "                            color=color,\n",
    "                            thickness=-1,\n",
    "                        )\n",
    "\n",
    "            # Apply the overlay with alpha blending for keypoints\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "            # Draw skeleton\n",
    "            for link_info in skeleton_info.values():\n",
    "                kp1_name, kp2_name = link_info[\"link\"]\n",
    "                kp1_id = next(\n",
    "                    (\n",
    "                        kp[\"id\"]\n",
    "                        for kp in keypoint_info.values()\n",
    "                        if kp[\"name\"] == kp1_name\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "                kp2_id = next(\n",
    "                    (\n",
    "                        kp[\"id\"]\n",
    "                        for kp in keypoint_info.values()\n",
    "                        if kp[\"name\"] == kp2_name\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                if kp1_id is not None and kp2_id is not None:\n",
    "                    if (\n",
    "                        frame_idx < len(keypoint_coords)\n",
    "                        and kp1_id < keypoint_coords.shape[1]\n",
    "                        and kp2_id < keypoint_coords.shape[1]\n",
    "                    ):\n",
    "                        x1, y1 = keypoint_coords[frame_idx, kp1_id]\n",
    "                        x2, y2 = keypoint_coords[frame_idx, kp2_id]\n",
    "                        kp1_conf = keypoint_conf[frame_idx, kp1_id]\n",
    "                        kp2_conf = keypoint_conf[frame_idx, kp2_id]\n",
    "                        color = tuple(link_info[\"color\"])\n",
    "                        alpha = min(\n",
    "                            kp1_conf, kp2_conf\n",
    "                        )  # Alpha value is the minimum confidence of the link\n",
    "                        if (\n",
    "                            kp1_conf > 0 and kp2_conf > 0 and not np.isnan(x1)\n",
    "                        ):  # Only draw if both confidence values are greater than 0\n",
    "                            overlay = cv2.line(\n",
    "                                overlay,\n",
    "                                (int(x1), int(y1)),\n",
    "                                (int(x2), int(y2)),\n",
    "                                color=color,\n",
    "                                thickness=2,\n",
    "                            )\n",
    "\n",
    "            # Apply the overlay with alpha blending for skeleton\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "            # Find centroid of bounding box\n",
    "            # x1, y1, x2, y2 = detection_coords[frame_idx, 0, :]\n",
    "            # centroid = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "            # overlay = cv2.circle(\n",
    "            #     overlay, centroid, radius=4, color=(0, 255, 0), thickness=-1\n",
    "            # )\n",
    "\n",
    "            # Draw the detection bounding box on the frame\n",
    "            # if frame_idx < len(detection_coords):\n",
    "            #     x1, y1, x2, y2 = detection_coords[frame_idx,0,:]\n",
    "            #     overlay = cv2.rectangle(\n",
    "            #         overlay, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2\n",
    "            #     )\n",
    "\n",
    "            # # Apply the overlay\n",
    "            alpha = 0.5\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "            # Write the frame with keypoints and skeletons to the output video\n",
    "            out.write(frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "            if max_frames and frame_idx >= max_frames:\n",
    "                break\n",
    "\n",
    "    # Release video objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b278082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_stich_vids(\n",
    "    output_directory: Path,\n",
    "    single_vid_suffix: str,\n",
    "    bbox_coords_by_camera: dict[np.ndarray]=None,\n",
    "    detection_coords_by_camera: dict[np.ndarray]=None,\n",
    "    bbox_crop_size=(400,400),\n",
    "    max_frames=None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Take keypoint videos and crop the mouse out, and stitch together the cropped videos into one row.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_directory : Path\n",
    "        Directory where the single videos will be found + output video will be saved.\n",
    "\n",
    "    single_vid_suffix : str\n",
    "        Suffix to identify the single videos to be stitched together.\n",
    "\n",
    "    bbox_coords_by_camera : dict or None\n",
    "        Dictionary containing the bounding box coordinates for each camera. The keys are camera names and the values are\n",
    "        numpy arrays of shape (#frames, 4) containing the bounding box coordinates (x1, y1, x2, y2) for each frame.\n",
    "        If None, must provide detection coordinates instead, which wil be treated as centroids.\n",
    "\n",
    "    detection_coords_by_camera : dict or None\n",
    "        Dictionary containing the detection coordinates for each camera. The keys are camera names and the values are\n",
    "        numpy arrays of shape (#frames, 4) containing the detection (ie centroid) coordinates (x, y) for each frame.\n",
    "        If None, must provide bbox coordinates instead, which will be used to infer a centroid + crop \n",
    "        (the bboxes from mmpose aren't uniform size, so we infer centroid + crop to standard size).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    assert bbox_coords_by_camera is not None or detection_coords_by_camera is not None, \"Must provide either bbox or detection coordinates.\"\n",
    "    assert bbox_coords_by_camera is None or detection_coords_by_camera is None, \"Must provide either bbox or detection coordinates, not both.\"\n",
    "\n",
    "    out_vids = list(output_directory.glob(f\"*{single_vid_suffix}.mp4\"))\n",
    "    timestamp, cam, vid_suffix = out_vids[0].stem.split(\".\")\n",
    "    stitched_vid_name = \".\".join([timestamp, \"stitched\", vid_suffix, \".mp4\"])\n",
    "    \n",
    "    # Get the total number of frames to use\n",
    "    tmp_cap = cv2.VideoCapture(str(out_vids[0]))\n",
    "    total_frames = int(tmp_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < 0 and max_frames is None:\n",
    "        raise ValueError(\n",
    "            \"Could not determine total number of frames in the video -- please specify max_frames.\"\n",
    "        )\n",
    "    elif total_frames < 0:\n",
    "        total_frames = max_frames\n",
    "    elif max_frames is not None:\n",
    "        total_frames = np.min([max_frames, total_frames])\n",
    "    tmp_cap.release()\n",
    "\n",
    "    # Calculate bbox centroids for the cropping\n",
    "    bbox_centroids_by_camera = {}\n",
    "    if bbox_coords_by_camera is not None:\n",
    "        for vid in out_vids:\n",
    "            recording_id, camera, frame, ext = os.path.basename(vid).split(\".\")\n",
    "            detn_coords = bbox_coords_by_camera[camera]\n",
    "            bbox_centroids_by_camera[camera] = np.array(\n",
    "                [\n",
    "                    [(x1 + x2) / 2, (y1 + y2) / 2]\n",
    "                    for x1, y1, x2, y2 in detn_coords\n",
    "                ]\n",
    "            )\n",
    "            # Apply median filter smoothing to reduce jitter\n",
    "            bbox_centroids_by_camera[camera] = median_filter(bbox_centroids_by_camera[camera], size=(12, 1))\n",
    "    elif detection_coords_by_camera is not None:\n",
    "        for vid in out_vids:\n",
    "            recording_id, camera, frame, ext = os.path.basename(vid).split(\".\")\n",
    "            bbox_centroids_by_camera[camera] = median_filter(detection_coords_by_camera[camera], size=(12, 1))\n",
    "\n",
    "\n",
    "    # Open the output video\n",
    "    out_vid_path = output_directory / Path(stitched_vid_name)\n",
    "    print(f\"Output video path: {out_vid_path}\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_frame_size = (bbox_crop_size[0] * len(out_vids), bbox_crop_size[1])\n",
    "    out = cv2.VideoWriter(str(out_vid_path), fourcc, 30, output_frame_size)\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "\n",
    "        # Open the input videos\n",
    "        cap_by_camera = {}\n",
    "        for vid in out_vids:\n",
    "            cap = cv2.VideoCapture(str(vid))\n",
    "            cap_by_camera[os.path.basename(vid).split(\".\")[1]] = cap\n",
    "        \n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            frames = []\n",
    "            for camera, cap in cap_by_camera.items():\n",
    "\n",
    "                # Read the frame\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Crop the frame\n",
    "                x, y = bbox_centroids_by_camera[camera][frame_idx]\n",
    "                x1, y1 = x - bbox_crop_size[0] // 2, y - bbox_crop_size[1] // 2\n",
    "                x2, y2 = x + bbox_crop_size[0] // 2, y + bbox_crop_size[1] // 2\n",
    "                frame = frame[int(y1) : int(y2), int(x1) : int(x2)]\n",
    "                frames.append(frame)\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Stitch the frames together\n",
    "            stitched_frame = np.zeros(\n",
    "                (bbox_crop_size[1], bbox_crop_size[0] * len(out_vids), 3), dtype=np.uint8\n",
    "            )\n",
    "            for i, frame in enumerate(frames):\n",
    "                stitched_frame[\n",
    "                    0 : frame.shape[0], i * frame.shape[1] : (i + 1) * frame.shape[1]\n",
    "                ] = frame\n",
    "            \n",
    "            # Write the stitched frame to the output video\n",
    "            out.write(stitched_frame)\n",
    "\n",
    "            # Loop control\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "            if max_frames and frame_idx >= max_frames:\n",
    "                break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec7084",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328d21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [\n",
    "    \"24-09-28-11-44-04-693209\",\n",
    "    \"24-09-29-12-40-04-238868\",\n",
    "    # \"24-09-30-15-43-47-490092\",  # one camera's 2d pred keeps failing\n",
    "    \"24-10-01-18-48-38-861115\",\n",
    "    \"24-10-04-14-07-59-928846\",\n",
    "    # \"24-10-08-17-56-50-878824\",  # missing triggerdata file\n",
    "\n",
    "]\n",
    "calibration_timestamps = [\n",
    "    \"24-09-28-12-57-16-037945\",\n",
    "    \"24-09-29-13-56-13-243339\",\n",
    "    # \"24-09-30-16-49-17-341423\"\n",
    "    \"24-10-01-20-49-29-153123\",\n",
    "    \"24-10-04-13-52-07-229882\",\n",
    "    # \"24-10-08-19-08-13-461143\",\n",
    "]\n",
    "recording_dir = (\n",
    "    \"/n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901\"  # path to raw videos\n",
    ")\n",
    "results_dir = \"/n/groups/datta/kpts_pipeline/tim_240731/results\"\n",
    "pred_2d_dir = join(results_dir, \"2D_predictions\")  # path to 2D kp predictions\n",
    "triang_3d_dir = join(results_dir, \"triangulation\")  # path to triangulated 3D kp predictions\n",
    "calibration_dir = join(results_dir, \"camera_calibration\")  # path to calibration files\n",
    "video_output_directory = Path(\n",
    "    join(recording_dir, \"tim_240731_keypoint_videos\")\n",
    ")  # path to save output videos\n",
    "video_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "max_frames = 120 * 20  # number of frames to process per video\n",
    "# max_frames = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abac363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61239a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP data\n",
    "\n",
    "sessions = [\n",
    "    \"20240723\",\n",
    "]\n",
    "recording_dir = (\n",
    "    \"/n/groups/datta/charlotte/DATA/Internal_state_MOSEQ/Male_03\"\n",
    ")\n",
    "\n",
    "video_output_directory = Path(\n",
    "    join(recording_dir, \"tim_240731_keypoint_videos\")\n",
    ")  # path to save output videos\n",
    "# video_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "max_frames = 120 * 60  # number of frames to process per video\n",
    "# max_frames = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830793a",
   "metadata": {},
   "source": [
    "# Generate the videos of the raw 2D predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a264dda",
   "metadata": {},
   "source": [
    "## Load the necessary data / info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c1ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset_name', 'paper_info', 'keypoint_info', 'skeleton_info', 'upper_body_ids', 'lower_body_ids', 'joint_weights', 'sigmas'])\n"
     ]
    }
   ],
   "source": [
    "# Using 25 kpt model for now\n",
    "from multicamera_airflow_pipeline.tim_240731.skeletons.sainburg25pt import dataset_info\n",
    "print(dataset_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d58b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in sessions:\n",
    "\n",
    "    prediction_files = glob(join(pred_2d_dir, session, f\"{session}*.h5\"))\n",
    "    \n",
    "    detection_coords_by_vid = {}\n",
    "    kp_coords_by_vid = {}\n",
    "    kp_conf_by_vid = {}\n",
    "    video_paths_by_vid = {}\n",
    "    session_dict[session] = {}\n",
    "    for h5_file in prediction_files:\n",
    "        \n",
    "        # Load the data\n",
    "        with h5py.File(h5_file, \"r\") as file:\n",
    "            keypoint_coords = np.array(file[\"keypoint_coords\"])  # shape: (n_frames, n_keypoints, 2)\n",
    "            keypoint_conf = np.array(file[\"keypoint_conf\"])\n",
    "            detection_conf = np.array(file[\"detection_conf\"])\n",
    "            detection_coords = np.array(file[\"detection_coords\"])\n",
    "        keypoint_conf[\n",
    "            keypoint_conf > 1\n",
    "        ] = 1  # not sure if this is the right way to fix this? Unhelpful discussion at https://github.com/open-mmlab/mmpose/issues/884\n",
    "\n",
    "        # TODO: make overall qc plots\n",
    "        # print(keypoint_coords.shape)\n",
    "        # print(keypoint_conf.shape)\n",
    "        # plt.figure()\n",
    "        # plt.matshow(keypoint_conf[:, 0, :].T, aspect=\"auto\")\n",
    "        # plt.title(\n",
    "        #     f'Keypoint confidence\\n{session}\\nCam {os.path.basename(h5_file).split(\".\")[1]}'\n",
    "        # )\n",
    "\n",
    "        # Find video\n",
    "        # NB: may need to change this per user depending on how / where the videos are stored\n",
    "        recording_id, camera, frame, ext = os.path.basename(h5_file).split(\".\")\n",
    "        assert frame == \"0\"\n",
    "        video_path = glob(join(recording_dir, \"**\", session, f\"*{camera}*.mp4\"))[0]\n",
    "\n",
    "        detection_coords_by_vid[camera] = detection_coords.squeeze()\n",
    "        kp_coords_by_vid[camera] = keypoint_coords.squeeze()\n",
    "        kp_conf_by_vid[camera] = keypoint_conf.squeeze()\n",
    "        video_paths_by_vid[camera] = video_path\n",
    "\n",
    "        \n",
    "    session_dict[session][\"2D_bbox_coords\"] = detection_coords_by_vid\n",
    "    session_dict[session][\"2D_kp_coords\"] = kp_coords_by_vid\n",
    "    session_dict[session][\"2D_kp_conf\"] = kp_conf_by_vid\n",
    "    session_dict[session][\"video_paths\"] = video_paths_by_vid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9e6a6",
   "metadata": {},
   "source": [
    "## Create QC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5a1057a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for session in sessions:\n",
    "\n",
    "    kp_confs = session_dict[session][\"2D_kp_conf\"]\n",
    "\n",
    "    all_kp_confs = np.stack([kp_confs[cam] for cam in kp_confs.keys()], axis=-1)\n",
    "    max_kp_confs_per_frame = np.max(all_kp_confs, axis=-1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.matshow(max_kp_confs_per_frame.T, aspect=\"auto\", cmap=\"PiYG\", vmin=0, vmax=1)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"Max keypoint confidence\")\n",
    "    cbar.set_ticks([0, 0.5, 1])\n",
    "    plt.ylabel(\"Keypoint\")\n",
    "    plt.title(f\"Max keypoint confidences across cameras\\n{session}\")\n",
    "    plt.savefig(join(video_output_directory, f\"{session}_max_kp_confs.png\"))\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228ac90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a794d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11a785b5",
   "metadata": {},
   "source": [
    "## Create the detection videos for each camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "68daa16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos already processed: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240928_J07901_6cam_PBN/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.BackBottom.0.mp4\n",
      "Videos already processed: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240928_J07901_6cam_PBN/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.BackLeft.0.mp4\n",
      "Videos already processed: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240928_J07901_6cam_PBN/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.BackRight.0.mp4\n",
      "Videos already processed: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240928_J07901_6cam_PBN/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.FrontBottom.0.mp4\n",
      "Videos already processed: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240928_J07901_6cam_PBN/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.FrontLeft.0.mp4\n",
      "Videos already processed: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/20240928_J07901_6cam_PBN/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.FrontRight.0.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372258f637584233aae78fcc6c512a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a7b7f613b445d18f600830a1c8bbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb62c2c30df42e38e9c47fe178d0568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackRight.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd809a189fc45269d0ce560cd9b604d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe73b9cdf5044538ea350f2b2c6a9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665e049425f647319c222f7697550a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontRight.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cc9156f0b5460fbc880a5ff48d006d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.BackBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2f1a73767946f191b5b9256e19bdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.BackLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1060b187eb23423a869cd2caa59812bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.BackRight.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adb7d9b9a0d4b9c96b05517e3f1f212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.FrontBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ad6026d51042c291dc1926663ad81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.FrontLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2f532d810d40949c48d3d3dccb5dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.FrontRight.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd51f6ca9c44e2d888fd03c4a4db253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.BackBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bc94d1a230466784d9da1243812d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.BackLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6548d8560b4dc98ec4067ac1de9036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.BackRight.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed87480a7bda4055a3d1529b2e04a921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.FrontBottom.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c787a492f0f4b1cbc90eec9ec4dbddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.FrontLeft.0_with_2D_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d81c28d78ae4a02a1c280346b5f4872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.FrontRight.0_with_2D_keypoints.mp4\n"
     ]
    }
   ],
   "source": [
    "for session in sessions:\n",
    "    session_output_directory = video_output_directory / session\n",
    "    session_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    vid_suffix = \"with_2D_keypoints\"\n",
    "    output_vids = list(session_output_directory.glob(f\"*{vid_suffix}.mp4\"))\n",
    "    if len(output_vids) == len(prediction_files):\n",
    "        print(f\"Videos already processed: {session_output_directory}\")\n",
    "        continue\n",
    "\n",
    "    for h5_file in prediction_files:\n",
    "        \n",
    "        recording, camera, frame, ext = os.path.basename(h5_file).split(\".\")\n",
    "\n",
    "        # Load the data\n",
    "        video_path = session_dict[session][\"video_paths\"][camera]\n",
    "        keypoint_coords = session_dict[session][\"2D_kp_coords\"][camera]\n",
    "        keypoint_conf = session_dict[session][\"2D_kp_conf\"][camera]\n",
    "        bbox_coords = session_dict[session][\"2D_bbox_coords\"][camera]\n",
    "\n",
    "        generate_keypoint_video(\n",
    "            output_directory=session_output_directory,\n",
    "            video_path=Path(video_path),\n",
    "            keypoint_coords=keypoint_coords,\n",
    "            keypoint_conf=keypoint_conf,\n",
    "            keypoint_info=dataset_info[\"keypoint_info\"],\n",
    "            vid_suffix=vid_suffix,\n",
    "            detection_coords=bbox_coords,\n",
    "            skeleton_info=dataset_info[\"skeleton_info\"],\n",
    "            max_frames=max_frames,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2dbc0f",
   "metadata": {},
   "source": [
    "## Crop and stitch the detection videos into one big one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d035be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.stitched.0_with_2D_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30e6bf22b0b40739fa9a76802b65d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.stitched.0_with_2D_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caca75599071444489a49a6d07340d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.stitched.0_with_2D_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4484f8aeb8c444eaad1ba4af6c3cfcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.stitched.0_with_2D_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871f0e69fec7405e8643d9cbd0daed14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for session in sessions:\n",
    "    session_output_directory = video_output_directory / session\n",
    "    bbox_coords_by_vid = session_dict[session][\"2D_bbox_coords\"]\n",
    "    crop_and_stich_vids(\n",
    "        output_directory=session_output_directory,\n",
    "        bbox_coords_by_camera=bbox_coords_by_vid,\n",
    "        single_vid_suffix=\"with_2D_keypoints\",\n",
    "        max_frames=max_frames,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5c4e0",
   "metadata": {},
   "source": [
    "# Generate the videos of the 3D triangulation predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d06e1f",
   "metadata": {},
   "source": [
    "## Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b793ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memmap_from_filename(filename):\n",
    "    # Extract the metadata from the filename\n",
    "    parts = filename.name.rsplit(\".\", 4)  # Split the filename into parts\n",
    "    dtype_str = parts[-3]  # Get the dtype part of the filename\n",
    "    shape_str = parts[-2]  # Get the shape part of the filename\n",
    "    shape = tuple(map(int, shape_str.split(\"x\")))  # Convert shape string to a tuple of integers\n",
    "    # Load the array using numpy memmap\n",
    "    array = np.memmap(filename, dtype=dtype_str, mode=\"r\", shape=shape)\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33cd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_preceding(arr):\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    result = arr.copy()\n",
    "\n",
    "    # Iterate over each element in the first dimension\n",
    "    for i in range(1, arr.shape[0]):\n",
    "        mask = np.isnan(result[i])  # Identify the NaN values\n",
    "        result[i][mask] = result[i - 1][mask]  # Replace NaNs with preceding values\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293894c1",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e360cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multicam_calibration as mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b11d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset_name', 'paper_info', 'keypoint_info', 'skeleton_info', 'upper_body_ids', 'lower_body_ids', 'joint_weights', 'sigmas'])\n"
     ]
    }
   ],
   "source": [
    "# Using 25 kpt model for now\n",
    "from multicamera_airflow_pipeline.tim_240731.skeletons.sainburg25pt import dataset_info\n",
    "print(dataset_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c049af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for session, calibration_timestamp in zip(sessions, calibration_timestamps):\n",
    "    prediction_files = glob(join(triang_3d_dir, session, \"predictions_3d*.mmap\"))\n",
    "    confidence_files = glob(join(triang_3d_dir, session, \"confidences_3d*.mmap\"))\n",
    "    reproj_err_files = glob(join(triang_3d_dir, session, \"reprojection_errors*.mmap\"))\n",
    "    \n",
    "    kp_reproj_coords_by_cam = {}\n",
    "    video_paths_by_cam = {}\n",
    "    session_dict[session] = {}\n",
    "    for pred_file, conf_file, reproj_err_file in zip(prediction_files, confidence_files, reproj_err_files):\n",
    "        \n",
    "        # Load the data\n",
    "        keypoint_coords = load_memmap_from_filename(Path(pred_file))\n",
    "        keypoint_confs = load_memmap_from_filename(Path(conf_file))  # n_frames, n_kpts\n",
    "        reproj_err = load_memmap_from_filename(Path(reproj_err_file))\n",
    "        \n",
    "        all_extrinsics, all_intrinsics, camera_names = mcc.load_calibration(\n",
    "            Path(join(calibration_dir, calibration_timestamp, \"jarvis\", \"CalibrationParameters\")).as_posix(),\n",
    "            load_format=\"jarvis\",\n",
    "        )\n",
    "\n",
    "        # TODO: make overall qc plots\n",
    "\n",
    "        # get reprojections  (n_frames, n_cams, n_kpts, 2)\n",
    "        positions_2D_reprojections = (\n",
    "            np.zeros((keypoint_coords.shape[0], len(camera_names), keypoint_coords.shape[1], 2)) * np.nan\n",
    "        )  \n",
    "        videos = glob(join(recording_dir, \"**\", session, \"*.0.mp4\"))\n",
    "        for vid_path in videos:\n",
    "            cam = os.path.basename(vid_path).split(\".\")[1]\n",
    "            calibration_cam_idx = camera_names.index(cam)\n",
    "            extrinsics = all_extrinsics[calibration_cam_idx]\n",
    "            camera_matrix, dist_coefs = all_intrinsics[calibration_cam_idx]\n",
    "            positions_2D_reprojections[:, calibration_cam_idx, :, :] = mcc.project_points(\n",
    "                keypoint_coords,\n",
    "                extrinsics=extrinsics,\n",
    "                camera_matrix=camera_matrix,\n",
    "                dist_coefs=dist_coefs,\n",
    "            )\n",
    "            \n",
    "            kp_reproj_coords_by_cam[cam] = positions_2D_reprojections[:, calibration_cam_idx, :, :]\n",
    "            video_paths_by_cam[cam] = vid_path\n",
    "        \n",
    "    session_dict[session][\"triang_kp_reproj_coords\"] = kp_reproj_coords_by_cam\n",
    "    session_dict[session][\"video_paths\"] = video_paths_by_cam\n",
    "    session_dict[session][\"triang_keypoint_confs\"] = keypoint_confs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c936555",
   "metadata": {},
   "source": [
    "## Create the 3D reprojection videos for each camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ce4a45ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38200b1699544c39a28d2c789b09ba48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.BackBottom.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810a661ffead461c964bae44f8654778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.BackLeft.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cae3967190a40a6a1b592e06b154329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.BackRight.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c9399c87674bada315923de831bcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.FrontBottom.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac66ebb644f2465abd08370f92dd16c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.FrontLeft.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd69f1c6a21d4bbdba0911a241e906e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.FrontRight.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11efee2e515d420d9b9a092f89c6d9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackBottom.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb6f12b49a54d8dac0835f9f7bf61a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackLeft.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bebde3f539453baa649edccddb0e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.BackRight.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707b412b3efd42bcbec77de1ec58c73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontBottom.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6716f28630f44169b0e19fb77026b008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontLeft.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49722966df2741c9a3668a700dd00d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.FrontRight.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876a927f432e46d6964dee7eb190ded0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.BackBottom.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2f82ff64b9492cbc634a9d9c63130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.BackLeft.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54e5d3aa5c6464b8fd28bcefdb68893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.BackRight.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2f648b55614dba98a189aeff7cd1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.FrontBottom.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd59bdbc91149648df7fa9741d65b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.FrontLeft.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40885f0f9fa47e59ce8e91bdd72d41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.FrontRight.0_with_triang_keypoints.mp4\n",
      "Total frames: 2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f2446ab88448bab00a637592e5f5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m reproj_keypoint_coords \u001b[38;5;241m=\u001b[39m session_dict[session][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtriang_kp_reproj_coords\u001b[39m\u001b[38;5;124m\"\u001b[39m][cam]\n\u001b[1;32m     10\u001b[0m keypoint_confs \u001b[38;5;241m=\u001b[39m session_dict[session][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtriang_keypoint_confs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgenerate_keypoint_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_output_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeypoint_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreproj_keypoint_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeypoint_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeypoint_confs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeypoint_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeypoint_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskeleton_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskeleton_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvid_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_triang_keypoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[138], line 181\u001b[0m, in \u001b[0;36mgenerate_keypoint_video\u001b[0;34m(output_directory, video_path, keypoint_coords, keypoint_conf, keypoint_info, skeleton_info, vid_suffix, detection_coords, max_frames)\u001b[0m\n\u001b[1;32m    172\u001b[0m             alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    173\u001b[0m                 kp1_conf, kp2_conf\n\u001b[1;32m    174\u001b[0m             )  \u001b[38;5;66;03m# Alpha value is the minimum confidence of the link\u001b[39;00m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    176\u001b[0m                 kp1_conf \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m kp2_conf \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x1)\n\u001b[1;32m    177\u001b[0m             ):  \u001b[38;5;66;03m# Only draw if both confidence values are greater than 0\u001b[39;00m\n\u001b[1;32m    178\u001b[0m                 overlay \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mline(\n\u001b[1;32m    179\u001b[0m                     overlay,\n\u001b[1;32m    180\u001b[0m                     (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)),\n\u001b[0;32m--> 181\u001b[0m                     (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(y2)),\n\u001b[1;32m    182\u001b[0m                     color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[1;32m    183\u001b[0m                     thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    184\u001b[0m                 )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Apply the overlay with alpha blending for skeleton\u001b[39;00m\n\u001b[1;32m    187\u001b[0m cv2\u001b[38;5;241m.\u001b[39maddWeighted(overlay, alpha, frame, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha, \u001b[38;5;241m0\u001b[39m, frame)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "for session in sessions:\n",
    "    session_output_directory = video_output_directory / session\n",
    "    session_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for cam in session_dict[session][\"triang_kp_reproj_coords\"].keys():\n",
    "        \n",
    "        # Grab the data\n",
    "        video_path = session_dict[session][\"video_paths\"][cam]\n",
    "        reproj_keypoint_coords = session_dict[session][\"triang_kp_reproj_coords\"][cam]\n",
    "        keypoint_confs = session_dict[session][\"triang_keypoint_confs\"]\n",
    "\n",
    "        generate_keypoint_video(\n",
    "            output_directory=session_output_directory,\n",
    "            video_path=Path(video_path),\n",
    "            keypoint_coords=reproj_keypoint_coords,\n",
    "            keypoint_conf=keypoint_confs,\n",
    "            keypoint_info=dataset_info[\"keypoint_info\"],\n",
    "            skeleton_info=dataset_info[\"skeleton_info\"],\n",
    "            vid_suffix=\"with_triang_keypoints\",\n",
    "            max_frames=max_frames,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6262c",
   "metadata": {},
   "source": [
    "## Crop and stitch the 3D reprojection videos into one big one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8838f257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22748/1854965631.py:10: RuntimeWarning: Mean of empty slice\n",
      "  centroids = np.nanmean(reproj_keypoint_coords, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-28-11-44-04-693209/24-09-28-11-44-04-693209.stitched.0_with_triang_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8211336cb29f476a962f04efc39c4e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-09-29-12-40-04-238868/24-09-29-12-40-04-238868.stitched.0_with_triang_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de40770d99fe4d238e88f13b9b7f2e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-01-18-48-38-861115/24-10-01-18-48-38-861115.stitched.0_with_triang_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c86817f83204e6aa1185c45dff64b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55ee8899a280] moov atom not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video path: /n/groups/datta/Jonah/20240925_PBN_npx/raw_data/J07901/tim_240731_keypoint_videos/24-10-04-14-07-59-928846/24-10-04-14-07-59-928846.stitched.0_with_triang_keypoints..mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67919c9dc55c460dbb4ac761547eafe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55ee88a97b80] moov atom not found\n"
     ]
    }
   ],
   "source": [
    "for session in sessions:\n",
    "    session_output_directory = video_output_directory / session\n",
    "    session_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Instead of using the raw detections, use the centroid of the triang'd kps, which will be cleaner.\n",
    "    # detection_coords_by_vid = session_dict[session][\"2D_detection_coords\"]\n",
    "    detection_coords_by_vid = {}\n",
    "    for cam in session_dict[session][\"triang_kp_reproj_coords\"].keys():\n",
    "        reproj_keypoint_coords = session_dict[session][\"triang_kp_reproj_coords\"][cam]\n",
    "        centroids = np.nanmean(reproj_keypoint_coords, axis=1)\n",
    "        centroids = nan_to_preceding(centroids)\n",
    "        detection_coords_by_vid[cam] = centroids\n",
    "\n",
    "    crop_and_stich_vids(\n",
    "        output_directory=session_output_directory,\n",
    "        detection_coords_by_camera=detection_coords_by_vid,\n",
    "        single_vid_suffix=\"with_triang_keypoints\",\n",
    "        max_frames=max_frames,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a5867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b654f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peromoseq",
   "language": "python",
   "name": "peromoseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
